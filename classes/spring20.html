<!DOCTYPE html>
<html>

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="keywords" content="Ferdinando Fioretto,Nando Fioretto,Nando,Ferdinando,Fioretto,DCOP,Smart Grid,AI,GPU,Distributed" />
	<meta name="author" content="Ferdinando Fioretto" />
	<link rel="shortcut icon" href="../img/su_icon.png" target="blank">
	<link href='https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css' />
	<link rel="stylesheet" type="text/css" media="all" href="../inland.css" />
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script type="text/javascript" src="js/jquery.nivo.slider.js"></script>
	<title>Ferdinando Fioretto</title>
	<script>
	$(document).ready(function(){
		var moreText = "Show more",
		lessText = "Show less",
		moreButton = $("a.read_more");

		moreButton.click(
			function () {
				var $this = $(this);
				$this.text($this.text() == moreText ? lessText : moreText).next("div.more").slideToggle("fast");
			}
			);
	});
	</script>
	<script>
	$(document).ready(function(){
		var moreText = "Show more",
		lessText = "Show less",
		moreButton = $("a.read_more_title");
		moreButton.click(
			function () {
				$(this).nextAll("div.more").slideToggle("fast");
				var $show_text = $(this).nextAll("a.read_more");
				$show_text.text($show_text.text() == moreText ? lessText : moreText)
			}
			);
	});
	</script>

<style>
table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #CCD1D1;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #EAECEE;
}
</style>
</head>

<body>
<div id="wrapper960" class="clearfix">
	<div id="toplinks">
	<ul class="toplinks_links">
	<li>
		<a href='https://eng-cs.syr.edu/our-departments/electrical-engineering-and-computer-science/'>
		<img align='center' src="../img/su_logo.png"  height="40pt" style="margin:-14px -20px -14px 0px">
		</a>

	</li>
	</ul>
	</div>

	<div id="header" class="clearfix shadow">
		<div id="sitetitle" class="clearfix">
			<h1>Ferdinando Fioretto</h1>
		</div>

		<div id="nav" class="clearfix">
		<ul>
			<li><a href="../../../index.html">FF's Home</a></li>
			<li><a href="syllabus.html">Syllabus</a></li>
			<li><a href="schedule.html">Schedule</a></li>
			<li><a href="#.html">Policies</a></li>
			<li><a href="#.html">Projects</a></li>
			<li><a href="#.html">Links</a></li>
		</ul>
		</div>
	</div>

	<!-- HEAD -->
	<div id="content" class="clearfix shadow">
		<h1> Security and Privacy of Machine Learning - CIS 700 / Spring 2020</h1>
	<div id="content">

		<h3>Overview</h3>
		<ul>
			<li>Instructor: Ferdinando Fioretto <a href="mailto:ffiorett@syr.edu">[email]</a></li>
			<li>Location: <a href="https://www.google.com/maps/place/Life+Sciences+Complex,+Syracuse,+NY+13210/@43.0381033,-76.1327506,17z/data=!3m1!4b1!4m5!3m4!1s0x89d9f3756db9604f:0xb3f780abea9df41e!8m2!3d43.0380994!4d-76.1305566" target="_blank">Life Science Building 300</a></li>
			<li>Time: Mon and Wed: 5:15-6:35pm</li>
			<li>Office hours: Fridays 12:30-1:30pm</li>
			<li>Office location: <a href="https://www.google.com/maps/place/Center+for+Science+and+Technology/@43.0374165,-76.1306932,21z/data=!3m1!5s0x89d9f3759f1f44fd:0xb825fcec7dd2d45c!4m5!3m4!1s0x89d9f3759f72e9a9:0xf55b7e60a321f302!8m2!3d43.0375369!4d-76.1306182" target="_blank">4-125 CST</a></li>
			<li>Initial Project Report Deadline: January 31</li>
			<li>Project Progress Report: February 28</li> 
		</ul>

		<h3>Teams</h3>
		<ul>
			<li><strong>Team Alderaan</strong>: Mu Bai </li>
			<li><strong>Team Coruscant</strong>: David Castello; Zuhal Altundal</li>
			<li><strong>Team Kamino</strong>: Joel Yuhas; Vedhas Sandeep Patkar</li>
			<li><strong>Team Mandalore</strong>: Amin Fallahi; Jindi Wu</li>
			<li><strong>Team Naboo</strong>: Ankit Khare; James Kotari</li>
			<li><strong>Team Onderon</strong>: Mengyu Liu; Lin Zhang</li>
			<li><strong>Team Tatooine</strong>: Pratik Ashok Paranjape; Raman Srivastava</li>
			<li><strong>Team Yavin</strong>: Cuong Tran; Hanyi Li</li>
<!-- 			<li><strong>Team Mustafar</strong>: Lin Zhang; Chirag Sachdev</li>
 -->		</ul>


		<h3>Schedule and material</h3>

		<p>Below is the calendar for this semester course. This is the preliminary schedule, which will be altered as the semester progresses. I will attempt to announce any change to the class, but this webpage should be viewed as authoritative. If you have any questions, please contact me.</p>


		<h4>Module 1: Evasion Attacks</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>

			<tbody>
			<tr>
			  <td>Jan 13</td>
			  <td>Overview & motivation</td> 
			  <td colspan="3"><a href="spring20/slides.pdf" target="_blank">slides</a></td>
			</tr>

			<!-- Lecture 1 -->
			<tr>
				<td>Jan&nbsp;15</td>
				<td>Attacks</td>
				<td><strong>Mandatory Reading</strong>:
				<ul>
					<li><em>D Lowd, C. Meek</em>. &quot;<a href="https://ix.cs.uoregon.edu/~lowd/ceas05lowd.pdf">Good Word Attacks on Statistical Spam Filters</a>&quot;</li>
					<li><em>N Dalvi et al.</em>. &quot;<a href="https://homes.cs.washington.edu/~pedrod/papers/kdd04.pdf">Adversarial classification</a>&quot;.</li>
					<li><em>C. Szegedy et al.</em>&quot;<a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>&quot;.</li>
				</ul>
				<strong>Optional Reading</strong>:
				<ul>
				  	<li><i>A. Kurakin, I. Goodfellow, S. Bengio</i>. &quot;<a href="https://arxiv.org/abs/1607.02533">Adversarial examples in the physical world</a>&quot;.</li>
					<li><em>N. Papernot, P. McDaniel, I. Goodfellow</em> &quot;<a href="https://arxiv.org/abs/1605.07277">Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples</a>&quot;.</li>
				</ul>
				</td>
				<td>Team&nbsp;Alderaan</td>
				<td>Team&nbsp;Naboo</td>
			</tr>

			<tr>
			  <td>Jan&nbsp;20</td>
			  <td colspan="4">No Class (Martin Luther Kind day)</td>
			</tr>

			<!-- Lecture 2 -->
			<tr>
			  <td>Jan&nbsp;22</td>
			  <td>Attacks and Adversarial Training</td>
			  <td><strong>Mandatory Reading</strong>: 
				<ul>
					<li><i>I Goodfellow et al.</i>.
						&quot;<a href="https://arxiv.org/abs/1412.6572">Explaining and harnessing adversarial examples</a>&quot; </li>
					<li><i>N Papernot, P McDaniel, S Jha, N Fredrikson, ZB Celik ZB, A Swami</i>.
						&quot;<a href="https://arxiv.org/abs/1511.07528">The Limitations of Deep Learning in Adversarial Settings</a>&quot;</li>
				</ul>
				<strong>Strongly suggested optional Reading</strong>:
				<ul>
					<li><i>F Tramèr, A Kurakin, N Papernot, I Goodfellow, D Boneh, P McDanie</i>.
						&quot;<a href="https://arxiv.org/abs/1705.07204">Ensemble Adversarial Training: Attacks and Defenses</a>&quot;</li>
				</ul>
				<strong>Optional Reading</strong>:
				<ul>
					<li><i>B. Biggio et al.</i>.
						&quot;<a href="http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/527.pdf">Evasion Attacks against Machine Learning at Test Time</a>&quot;.</li>
				</ul>
				<strong>Tutorial software</strong>:
				<ul>
					<li><a href="https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_tutorial_pytorch.py">Cleverhans mnist tutorial (Pytorch)</a></li>
				</ul>
			  </td>
  			<td>Team&nbsp;Coruscant</td>
			<td>Team&nbsp;Naboo</td>
			</tr>

			<!-- Lecture 3 -->
			<tr>
			  <td>Jan&nbsp;27 </td>	
			  <td>Defensive Distillation</td>
			  <td>
				<strong>Mandatory Reading</strong>: 
				<ul>
					<li><em>N Papernot et al.</em>.
						&quot;<a href="https://arxiv.org/abs/1511.04508">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</a>&quot;
					</li>
					<li><em>N Papernot, P McDaniel</em>.
						&quot;<a href="https://arxiv.org/abs/1607.05113">On the effectiveness of defensive distillation</a>&quot;
					</li>
				</ul>
				<strong>Strongly suggested Optional Reading</strong>:
				<ul>
					<li><em>A Athalye, N Carlini, D Wagner</em>.
						&quot;<a href="https://arxiv.org/abs/1802.00420">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</a>&quot;
					</li>
				</ul>
				<strong>Optional Reading</strong>:
				<ul>
					<li><em>G Hinton, O Vinyals, J Dean</em>.
					&quot;<a href="https://arxiv.org/abs/1503.02531">Distilling the knowledge in a neural networks</a>&quot;
					</li>
					<li><em>Siyue Wang et al.</em>
					&quot;<a href="https://arxiv.org/abs/1809.05165">Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks</a>&quot;
				</ul>
				</td>
				<td>Team&nbsp;Kamino</td>
				<td>Team&nbsp;Naboo</td>
			</tr>

			<!-- Lecture 4 -->
			<tr>
			  <td>Jan&nbsp;29</td>	
			  <td>Other Defensive Mechanisms</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
					<li><em>N Papernot et al.</em>
						&quot;<a href="https://dl.acm.org/doi/10.1145/3052973.3053009">Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples</a>&quot;
					</li>
					<li><em>J Jo, Y Bengio</em>
						&quot;<a href="https://arxiv.org/abs/1711.11561">Measuring the tendency of CNNs to Learn Surface Statistical Regularities</a>&quot;
					</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
					<li><em>C Guo et al.</em>
						&quot;<a href="https://arxiv.org/abs/1711.00117">Countering Adversarial Images using Input Transformations</a>&quot;
					</li>
			  	</ul>
			  </td>
	  		  <td>Team&nbsp;Mandalore</td>
		      <td>Team&nbsp;Naboo</td>
			</tr>
		</tbody>
		</table>

		<h4>Module 2: Poisoning Attacks</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>

			<tbody>
			<tr>
			  <td>Feb&nbsp;3 </td>	
			  <td>Introduction</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
  				<li><em>B Nelson et al</em>.
					<a href="https://people.eecs.berkeley.edu/~tygar/papers/SML/Spam_filter.pdf">&quot;Exploiting Machine Learning to Subvert Your Spam Filter</a>.&quot;
				</li>
  				<li><em>A Shafahi et al</em>.
  					<a href="https://arxiv.org/abs/1804.00792">&quot;Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks</a>.&quot;
				</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>X. Huang et al</i>.
			  			&quot;
			  			<a href="https://arxiv.org/abs/1804.07933" target="_blank">Is feature selection secure against training data poisoning?</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>
			  <td>Team&nbsp;Onderon</td>
			  <td>Team&nbsp;Coruscant</td>
			</tr>

			<tr>
			  <td>Feb&nbsp;5 </td><td>Attacks on ML systems</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
					<li><em>B Biggio, B Nelson, P Laskov</em>.
						&quot;<a href="https://arxiv.org/abs/1206.6389">Poisoning attacks against support vector machines</a>&quot;.
					</li>
					<li><em>M Jagielski et al</em>.
						&quot;<a href="https://arxiv.org/abs/1804.00308">Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning</a>&quot;.
					</li>
				</ul>
				<strong>Optimal Reading</strong>:
				<ul>
					<li><em>C Yudong, C Caramanis, S Mannor</em>.
						&quot;<a href="http://proceedings.mlr.press/v28/chen13h.html">Robust sparse regression under adversarial corruption</a>&quot;.
					</li>
				</ul>
			  </td>
  			  <td>Team&nbsp;Tatooine</td>
			  <td>Team&nbsp;Coruscant</td>
			</tr>

			<tr>
			  <td>Feb&nbsp;10 </td>	<td colspan="4">No Class (AAAI) </td>
			</tr>

			<tr>
			  <td>Feb&nbsp;12 </td><td colspan="4">No Class (AAAI) </td>
			</tr>

			<tr>
			  <td>Feb&nbsp;17 </td><td>Defense Mechanisms</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
					<li><em>B Rubinstein et al</em>.
						&quot;<a href="https://people.eecs.berkeley.edu/~tygar/papers/SML/IMC.2009.pdf">ANTIDOTE: Understanding and Defending against Poisoning of Anomaly Detectors</a>&quot;.
					</li>
					<li><em>J Steinhardt, PW Koh, P Liang</em>.
						&quot;<a href="https://papers.nips.cc/paper/6943-certified-defenses-for-data-poisoning-attacks.pdf">Certified Defenses for Data Poisoning Attack</a>&quot;.
					</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><em>B Wang et al</em>.
						&quot;<a href="https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf">Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks</a>&quot;.
			  		</li>
			  	</ul>
			  </td>
			  <td>Team&nbsp;Yavin</td>
			  <td>Team&nbsp;Coruscant</td>
			</tr>

		</tbody>
		</table>


		<h4>Module 3: Privacy Attacks</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>
			<tr>
			  <td>Feb&nbsp;19 </td>
			  <td>Data Exposure</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><em>A Narayanan, V Shmatikov</em>.
					&quot;<a href="https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf">Robust de-anonymization of large sparse datasets.</a>&quot;.
					</li>
					<li><em>F. Tramèr et al</em>.
						&quot;<a href="https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf">Stealing Machine Learning Models via Prediction APIs</a>&quot;.
					</li>
				</ul>
			  	<strong>Optional but Highly Encouraged Reading</strong>: 
			  	<ul>
			  		<li><em>C Dwork et al</em>.
					&quot;<a href="https://privacytools.seas.harvard.edu/files/privacytools/files/pdf_02.pdf">Exposed! A Survey of Attacks on Private Data</a>&quot;.
					</li>
				</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><em>JA Calandrino et al</em>.
					&quot;<a href="https://www.cs.utexas.edu/~shmat/shmat_oak11ymal.pdf">‘You Might Also Like’: Privacy Risks of Collaborative Filtering</a>&quot;.
					</li>
					<li><em>M Fredrikson et al</em>.
					&quot;<a href="https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf">Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing</a>&quot;.
			  	</ul>
			  </td>
  			  <td>Team&nbsp;Naboo</td>
			  <td>Team&nbsp;Alderaan</td>
			</tr>

			<tr>
			  <td>Feb&nbsp;24 </td><td>Privacy Attacks in Deep Learning</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><em>R Shokri et al</em>.
			  			&quot;<a href="https://arxiv.org/abs/1610.05820" target="_blank">Membership Inference Attacks against Machine Learning Models</a>&quot;.
			  		</li>
			  		<li><em>N Carlini et al</em>.
			  			&quot;<a href="https://arxiv.org/abs/1802.08232" target="_blank">The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. </a>&quot;.
			  		</li>
			  		<li><em>C Song, V Shmatikov</em>.
			  			&quot;<a href="https://arxiv.org/abs/1905.11742" target="_blank">Overlearning Reveals Sensitive Attributes.</a>&quot;.
			  		</li>

			  	</ul>
			  	<strong>Optional but Encouraged Reading</strong>:
			  	<ul>
			  		<li><em>M Nasr, R Shokri, A Houmansadr</em>.
			  			&quot;<a href="https://www.comp.nus.edu.sg/~reza/files/Shokri-SP2019.pdf" target="_blank">Comprehensive Privacy Analysis of Deep Learning</a>&quot;.
			  		</li>
			  	</ul>
			  </td>
  			  <td>Team&nbsp;Coruscant</td>
			  <td>Team&nbsp;Alderaan</td>
			</tr>
			<tr>
			  <td>Feb&nbsp;26 </td><td colspan="4">No Class (CRA) </td>
			</tr>
		</tbody>
		</table>

		<h4>Module 4: Differential Privacy</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>
			<tr>
			  <td>Mar&nbsp;2</td>
			  <td>Preliminaries</td>
			  <td>
				<strong>Mandatory Reading</strong>: 
			  	<ul> 
				<li><em>C Dwork, A Roth</em>.
				&quot;<a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">The Algorithmic Foundations of Differential Privacy</a>&quot;. 
  				Chapters 2 and 3
  				</li>
				<li><em>C Dwork et al</em>.
				&quot;<a href="https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf">Calibrating noise to sensitivity in private data analysis</a>&quot;.
				</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul> 
				<li><em>F McSherry, I Mironov</em>.
				&quot;<a href="">Differentially Private Recommender Systems: Building Privacy into the Netflix Prize Contenders</a>&quot;.				
				</li>	
				<li><em>I Mironov</em>.
				<a href="https://arxiv.org/abs/1702.07476">Renyi differential privacy</a>&quot;.
				</li>
				<!-- https://www.cisco.com/c/dam/en_us/about/doing_business/trust-center/docs/dpbs-2019.pdf -->
			  	</ul>
			  </td>
  			  <td>Guest Lecture: Nando</td>
			  <td>Team&nbsp;Kamino</td>
			</tr>

			<tr>
			  <td>Mar&nbsp;4</td>
			  <td>Data Generation</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>J Zhang et al</i>.
			  			&quot;
			  			<a href="http://dimacs.rutgers.edu/~graham/pubs/papers/PrivBayes.pdf" target="_blank">PrivBayes: Private Data Release via Bayesian Networks</a>
			  			&quot;.
			  		</li>
			  		<li><i>G. Cormode et al</i>.
			  			&quot;
			  			<a href="http://dimacs.rutgers.edu/~graham/pubs/papers/spatialpriv.pdf" target="_blank">Differentially private spatial decompositions</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>C Liand, G Miklau</i>.
			  			&quot;
			  			<a href="http://vldb.org/pvldb/vol5/p514_chaoli_vldb2012.pdf" target="_blank">
			  			An adaptive mechanism for accurate query answering under differential privacy</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>
			  <td>Team&nbsp;Mandalore</td>
			  <td>Team&nbsp;Kamino</td>
			</tr>

			<tr>
			  <td>Mar&nbsp;9</td>
			  <td>Optimization</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>H Rastogi, M Suciu.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/0904.0942" target="_blank">Boosting the Accuracy of Differentially Private Histograms Through Consistency</a>
			  			&quot;.
			  		</li>
			  		<li><i>B Ding et al.</i>:&nbsp;
			  			&quot;
			  			<a href="https://faculty.ist.psu.edu/jessieli/Publications/sigmod11_bding.pdf" target="_blank">Differentially private data cubes: optimizing noise sources and consistency</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>C Li et al.</i>:&nbsp;
			  			&quot;
			  			<a href="https://people.cs.umass.edu/~miklau/assets/pubs/dp/li2014data.pdf	" target="_blank">A Data- and Workload-Aware Algorithm for Range Queries Under Differential Privacy</a>
			  			&quot;.
			  		</li>
			  		<li><i>F Fioretto P Van Hentenryck.</i>:&nbsp;
			  			&quot;
			  			<a href="https://www2.isye.gatech.edu/~fferdinando3/files/papers/cp19.pdf" target="_blank">Differential Privacy of Hierarchical Census Data: An Optimization Approach</a>
			  			&quot;.
			  		</li>
			  		<li><i>T Mak, F Fioretto, P Van Hentenryck.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/1910.04250" target="_blank">Privacy-Preserving Obfuscation for Distributed Power Systems</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>
  			  <td>Team&nbsp;Tatooine</td>
			  <td>Team&nbsp;Kamino</td>
			</tr>
		</tbody>
		</table>

		<h4>Module 5: Differential Privacy and Machine Learning</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>
			<tr>
			  <td>Mar&nbsp;11 </td>
			  <td>Machine Learning</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>+ A Friedman, A Schuster</i>:&nbsp;
			  			&quot;
			  			<a href="http://dimacs.rutgers.edu/~graham/pubs/papers/PrivBayes.pdf" target="_blank">Data Mining with Differential Privacy</a>
			  			&quot;.
			  		</li>
			  		<li><i>+ K Chaudhuri, C Monteleoni, AD Sarwate. </i>:&nbsp;
			  			&quot;
			  			<a href="http://www.jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf" target="_blank">Differentially private empirical risk minimization</a>
			  			&quot;.
			  		</li>

			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>+ G Jagannathan, K Pillaipakkamnatt, RN Wright</i>:&nbsp;
			  			&quot;
			  			<a href="http://www.tdp.cat/issues11/tdp.a082a11.pdf" target="_blank">A Practical Differentially Private Random Decision Tree Classifier</a>
			  			&quot;.
			  		</li>
			  		<li><i>+ D Kifer, AD Smith, A Thakurta. </i>:&nbsp;
			  			&quot;
			  			<a href="http://proceedings.mlr.press/v23/kifer12/kifer12.pdf		" target="_blank">Private convex optimization for empirical risk minimization with applications to high-dimensional regression</a>
			  			&quot;.
			  		</li>
			  		<li><i>+ B Rubinstein et al.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/0911.5708" target="_blank">Learning in a large function space: Privacy-preserving mechanisms for SVM learning</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>

  			  <td>Team&nbsp;Yavin</td>
			  <td>Team&nbsp;Mandalore</td>			  
			</tr>

			<tr>
			  <td>Mar&nbsp;16 </td>
			  <td colspan="4">No Class (Spring Break)</td>
			</tr>
			
			<tr>
			  <td>Mar&nbsp;18 </td>
			  <td colspan="4">No Class (Spring Break)</td>
			</tr>
			</tr>

			<tr>
			  <td>Mar&nbsp;23 </td>
			  <td>Deep Learning</td>	
		
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>M Abadi et al.</i>
			  			&quot;
			  			<a href="#https://arxiv.org/abs/1607.00133" target="_blank">Deep learning with differential privacy</a>
			  			&quot;.
			  		</li>
			  		<li><i>N Papernot et al.</i>
			  			&quot;
			  			<a href="https://arxiv.org/abs/1610.05755" target="_blank">Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>N Papernot et al.</i>
			  			&quot;
			  			<a href="https://arxiv.org/abs/1802.08908" target="_blank">Scalable Private Learning with PATE </a>
			  			&quot;.
			  		</li>
			  		<li><i>N Phan et al.</i>
			  			&quot;
			  			<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12174" target="_blank">Differential Privacy Preservation for Deep Auto-Encoders: An Application of Human Behavior Prediction</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  	<strong>Software</strong>:
				<ul>
					<li><a href="http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html">cleverhans</a</li>
					<li><a href="https://github.com/tensorflow/privacy">TensorFlow privacy</a></li>
				</ul>
			  </td>
  			  <td>Team&nbsp;Alderaan + Nando</td>
			  <td>Team&nbsp;Mandalore</td>			  
			</tr>

			  <td>Mar&nbsp;25 </td>
			  <td>Generative Adversarial Networks</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>Gergely Acs et al.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/1709.04514" target="_blank">Differentially Private Mixture of Generative Neural Networks</a>
			  			&quot;.
			  		</li>
			  		<li><i>L Xie et al.</i>:&nbsp;
			  			&quot;
			  			<a href="Differentially Private Generative Adversarial Network" target="_blank">https://arxiv.org/abs/1802.06739</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>X Zhang, S Ji, T Wang.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/pdf/1801.01594.pdf" target="_blank">Differentially Private Releasing via Deep Generative Model (Technical Report)</a>
			  			&quot;.
			  		</li>
			  		<li><i>C Arnold, M Neunhoeffer, S Sternberg</i>:&nbsp;
			  			&quot;
			  			<a href="https://sebastiansternberg.github.io/pdf/An_Empirical_Evaluation_of_DP_GANs_for_Social_Science.pdf" target="_blank">An Empirical Evaluation of Differentially Private Generative Adversarial Networks for Science</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>
  			  <td>Team&nbsp;Onderon</td>
			  <td>Team&nbsp;Mandalore</td>
			</tr>
		</tbody>
		</table>

		<h4>Project Review Session</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>

			<tr>
			  <td>Mar&nbsp;30 </td>
			  <td>Project Review</td>
			  </td>
			  <td></td>
  			  <td>All</td>
		  	  <td></td>	
			</tr>
		</tbody>
		</table>

		<h4>Module 6: Differential Privacy Model Extensions</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>

			<tr>
			  <td>Apr&nbsp;1 </td>
			  <td>Local DP</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>G Cormode et al.</i>:&nbsp;
			  			&quot;
			  			<a href="http://dimacs.rutgers.edu/~graham/pubs/papers/ldptutorial.pdf" target="_blank">Privacy at Scale: Local Differential Privacy in Practice</a>
			  			&quot;.
			  		</li>
			  		<li><i>U Erlingsson, V Pihur, A Korolova.</i>:&nbsp;
			  			&quot;
			  			<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf" target="_blank">RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>B Ding, J Kulkarni, S Yekhanin</i>:&nbsp;
			  			&quot;
			  			<a href="https://papers.nips.cc/paper/6948-collecting-telemetry-data-privately.pdf" target="_blank">Collecting telemetry data privately		</a>
			  			&quot;.
			  		</li>
			  		<li><i>P Kairouz, S Oh, P Viswanath.</i>:&nbsp;
			  			&quot;
			  			<a href="https://papers.nips.cc/paper/5392-extremal-mechanisms-for-local-differential-privacy.pdf" target="_blank">Extremal Mechanisms for Local Differential Privacy</a>
			  			&quot;.
			  		</li>
			  		<li><i>B Bebensee.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/pdf/1907.11908.pdf" target="_blank">Local Differential Privacy: a tutorial</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>
  			  <td>Team&nbsp;Kamino</td>
			  <td>Team&nbsp;Tatooine</td>
			</tr>

		 	<td>Apr&nbsp;6 </td>
		 	<td>Temporal DP</td>
			<td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>V Rastogi, S Nath. </i>:&nbsp;
			  			&quot;
			  			<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2009/11/paper.pdf" target="_blank">Differentially private aggregation of distributed time-series with transformation and encryption</a>
			  			&quot;.
			  		</li>
			  		<li><i>F Fioretto, P Van Hentenryck.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/1808.01949" target="_blank">OptStream: Releasing Time Series Privately</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>Y Cao et al</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/pdf/1711.11436.pdf" target="_blank">Quantifying Differential Privacy in ContinuousData Release under Temporal Correlations</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>
  			  <td>Team&nbsp;Naboo</td>
			  <td>Team&nbsp;Tatooine</td>
			</tr>
		</tbody>
		</table>

		<h4>Module 7: Model Robustness</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>
			<tr>
			  <td>Apr&nbsp;8 </td><td>Robustness in ML</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>N Carlini, D Wagner </i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/1608.04644" target="_blank">Towards evaluating the robustness of neural networks.</a>
			  			&quot;
			  		</li>
			  		<li><i>Lecuyer et al., </i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/1802.03471" target="_blank">Certified Robustness to Adversarial Examples with Differential Privacy.</a>
			  			&quot;
			  		</li>

			  		<li>
						<i>Diakonikolas et al.,</i>&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/pdf/1803.02815.pdf" target="_blank">
						Sever: A Robust Meta-Algorithm for Stochastic Optimization.</a>
						&quot;
			  		</li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>M Hein, M Andriushchenko</i>:&nbsp;
			  			&quot;
			  			<a href="https://papers.nips.cc/paper/6821-formal-guarantees-on-the-robustness-of-a-classifier-against-adversarial-manipulation" target="_blank">Formal guarantees on the robustness of a classifier against adversarial manipulation”</a>
			  			&quot;.
			  		</li>
			  		<li><i>N Carlini et al.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/abs/1709.10207" target="_blank">Provably Minimally-Distorted Adversarial Examples</a>
			  			&quot;.
			  		</li>
			  	</ul>
			  </td>
			  <td>Team&nbsp;Coruscant</td>
			  <td>No Notes</td>
			</tr>
		</tbody>
		</table>

		<h4>Module 8: Federated Learning</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>
			<tr>
			  <td>Apr&nbsp;13 </td><td>Preliminaries</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>Kairouz et al.</i>:&nbsp;
			  			&quot;
			  			<a href="https://arxiv.org/pdf/1912.04977.pdf">Advances and Open Problems in Federated Learning</a>
			  			&quot;</li>
 			  </ul>
			  </td>
			  <td>Guest Lecture: Pranay</td>
			  <td>Team&nbsp;Onderon</td>
			</tr>

			<tr>
			  <td>Apr&nbsp;15 </td><td>Seminal papers</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>McMahan et al.</i><a href="http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf">Communication-Efficient Learning of Deep Networks from Decentralized Data</a></li>
			  		<li><i>Bagdasaryan, et al.</i> <a href="https://arxiv.org/pdf/1807.00459.pdf"> How To Backdoor Federated Learning</a></li>
			  	</ul>
			  	<strong>Optional Reading</strong>:
			  	<ul>
			  		<li><i>Li, et al.</i>: <a href="https://arxiv.org/pdf/1812.06127.pdf">Federated Optimization in Heterogeneous Networks</a>.</li>
			  		<li><i>Unkown</i><a href="https://openreview.net/pdf?id=BkluqlSFDS">Federated learning with matched averaging</a></li>
			  	</ul>
			  </td>
			  <td>Team&nbsp;Tatooine</td>
			  <td>Team&nbsp;Onderon</td>
			</tr>

			<tr>
			  <td>Apr&nbsp;20</td>
			  <td>Privacy</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
			  		<li><i>Geyer et al.</i><a href="https://arxiv.org/pdf/1712.07557.pdf">Differentially private federated learning: a client level perspective</a></li>
			  		<li>McMahan<a href="https://arxiv.org/pdf/1710.06963.pdf">Learning differentially private recurrent language models</a></li>
			  </ul>
			  	<strong>Optional Reading</strong>:
			  		<li><i>Liang et al.</i>: <a href="https://arxiv.org/pdf/2001.01523v1.pdf">Think Locally, Act Globally: Federated Learning with Local and Global Representations.</a></li>
			  </td>
  			  <td>Team&nbsp;Yavin</td>
			  <td>Team&nbsp;Onderon</td>
			</tr>
		</tbody>
		</table>

		<h4>Module 9: Fairness and Bias</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>
			<tr>
			  <td>Apr&nbsp;22</td>
			  <td>Preliminaries</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
			  	<ul>
		  		<li><i>Dwork et al.</i><a href="https://arxiv.org/abs/1104.3913">
		  		Fairness Through Awareness</a>
			  	</li>

			  	<li><i>Goh et al.</i>
			  		<a href="http://papers.nips.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints.pdf">Satisfying Real-world Goals with Dataset Constraints</a>
			  	</li>
			  </ul>

			  	<strong>Optional Reading</strong>:
			  	<ul>		  		
			  		<li><i>M. Feldman et al.</i><a href="https://arxiv.org/abs/1412.3756">
		  			Certifying and removing disparate impact</a>
			  		</li>
				</ul>
			  </td>
  			  <td>Team&nbsp;Mandalore</td>
			  <td>Team&nbsp;Yavin</td>
			</tr>

			<tr>
			  <td>Apr&nbsp;27 </td><td>Applications</td>
			  <td>
			  	<strong>Mandatory Reading</strong>: 
		  		<li><i>Zafar et al.</i><a href="https://people.mpi-sws.org/~gummadi/papers/disparate_impact_AISTATS_2017.pdf">Fairness Constraints: Mechanisms for Fair Classification</a></li>
		  		<li>
		  			<i>Liu et al.</i>
		  			<a href="https://arxiv.org/abs/1803.04383">Delayed Impact of Fair Machine Learning</a>
		  		</li>
		  		<strong>Optional Reading</strong>:
			  	<ul>
			  		<li>
			  			Y. Bechavod, K. Ligett<a href="https://arxiv.org/pdf/1707.00044.pdf">Penalizing Unfairness in Binary Classification</a>
			  		</li>
			  		<li>
			  			<i>Hardt et al.</i>
			  			<a href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">Equality of Opportunity in Supervised Learning</a>
			  		</li>
			  	</ul>
			  </td>
  			  <td>Team&nbsp;Kamino</td>
			  <td>Team&nbsp;Yavin</td>
			</tr>
		</tbody>
		</table>

		<h4>Final Presentation</h4>
		<table class="table">
			<thead>
			<tr style="font-weight:bold">
			  <th scope="col">Date</th>
			  <th scope="col">Topic</th>
			  <th scope="col">Reading / Assignment</th>
			  <th scope="col">Present</th>
			  <th scope="col">Report</th>
			</tr>
			</thead>
			<tbody>

			<tr>
			  <td>May&nbsp;4</td><td colspan="4">Poster Session</td>
			</tr>


			</tbody>
		</table>

		<h3>Assignments</h3>

		<p><h5>LaTeX template for assignments</h5>
			All class notes should be submitted using the <a href="https://aaai.org/Press/Author/authorguide.php">AAAI Template</a>.
		</p>

		<p><h5>Paper presentation</h5> 
		In each class, a team of students will present the assigned papers. 
		Different types of presentation are allowed (e.g., slides, interactive demos or code tutorials). The only requirements is that the presentation should (a) involve the class in active discussions, (b) cover all papers assigned for reading, and (c) last no more than 1:15h including discussions. </p>

		<p><h5>Class notes</h5> 
		Another team of students will be charged with writing notes synthesizing the content of the presentation and class discussion.</p>

		<p><h5>Research projects</h5> 
		Students will work on a course-long research project. Each project will be presented in the form of a poster on May 4 (tentative).</p>

		<h3>Grading</h3>

		<p><h5>Grading scheme</h5> 
		30% paper presentation, 20% class notes, 10% class participation, 40% research project.</p>

		<p><h5>Class participation</h5> 
		Course lectures will be driven by the contents of assigned papers. All students are asked to participate in an active discussions of the paper content during each class. 
		</p>

		<p><h5>Lateness policy</h5> 
		The presentation material should be presented two days prior the day of presentation.
		A 10% per-day late-penalty will be applied for delays. 
		If the presentation is not ready for the day in which the team is supposed to present all students in the team will be assigned 0 points.
		</p>

		<p><h5>Integrity</h5> 
		Any instance of sharing or plagiarism, copying, cheating, or other disallowed behavior will constitute a breach of ethics. Students are responsible for reporting any violation of these rules by other students, and failure to constitutes an ethical violation that carries with it similar penalties.</p>

		<h3>Ethics statement</h3>

		<p>In this course, you will be learning about and exploring some vulnerabilities that could be exploited to compromise deployed systems. You are trusted to behave responsibility and ethically. You may not attack any system without permission of its owners, and may not use anything you learn in this class for evil. If you have doubts about ethical and legal aspects of what you want to do, you should check with the course instructor before proceeding.
		Any activity outside the letter or spirit of these guidelines will be reported to the proper authorities and may result in dismissal from the class.</p>

		<div name="projects" id="projects">
		<h3>Final Projects Summary</h3>


		<h4> Automotive Anomaly Detection with Resistence to Adversarial Samples
		</h4>
		<p>
			<strong>Members</strong>: Mengyu Liu, Lin Zhang<br>
			<strong>Summary</strong>: 
			Cyber-Physical system is a field integrate communication, controlling and computing with multiple sensors and actuators connected to the physical world. Attacks from invader and complex environment could lead to abnormal state of the system. Many previous works designed various anomaly detection algorithms to handle this problem. Recently, machine learning methods has been applied in many works but anomaly samples are extreme rare which makes the performance not good. An efficiency way is data augmentation, a few works applied Generative neural networks to achieve better performance, but none of them explain how the process related to physical world and did interpret it from a knowledge-based view. Our work proposed a novel perspective on how to understand how Generative Neural Network helped exploiting the knowledge of our collected data with different anomaly detection algorithm. Additionally, we built testbeds to simulate autonomous vehicles to see how the augmented data related to physical world.
			<br>
			<a href="spring20/Projects/Mengyu_Lin_presentation.pptx">Presentation</a>
		</p>
<!-- 
		<h4>Membership Inference Attacks Against Thermal 
		Image Classification Models for Stress Detection</h4>

		<p>
			<strong>Members</strong>: Amin Fallahi
			<br>
			<strong>Summary</strong>:
			 Membership Inference Attacks have proved to be successful against different machine learning models. These attacks focus  on  determining  if  particular  records  appear  in  the  data that has been used in training the model. As a result, leaking important information about the record. One application of these attacks is to exploit useful information about the patients who have been participated in thermal image collection for medical studies. With access to the identity of the patients,the adversaries can extract sensitive information about them.In this work, we train a model for stress detection using thermal images and launch membership inference attacks against the model. Then, we evaluate our work to see how much the attack is successful in extracting useful information about the participants whose data has been used in model training.
			 <br>
			<a href="spring20/Projects/amin_slides.pdf">Presentation</a>
		</p> -->

		<h4> Comparing Model Accuracy for Differential Privacy and Machine Unlearning on Sensitive Data
		</h4>
		<p>
			<strong>Members</strong>: Vedhas S. Patkar and Joel W. Yuhas <br>
			<strong>Summary</strong>:
			Machine Learning models, particularly those that employ neural networks, generally need large datasets for identification.  Our  aim  is  to  employ  existing  models that use differential privacy and machine unlearning to determine the best possible outcomes for sensitive and identifiable data, both in terms of privacy and model accuracy. We will set out to find the limitations and advantages of existing architectures, and figure out the possible changes that can be made to them to provide better outcomes
			<br>
			<a href="spring20/Projects/Vedhas_Presentation.pdf">Presentation</a>
		</p>

		<h4>
			Asynchronous Federated Learning - Literature Review
		</h4>
		<p>
			<strong>Members</strong>: David Castello <br>
			<strong>Summary</strong>:
			Federated learning is an emergent field of research that seeks to train performant machine learning models across a heterogeneous collection of mobile devices, each with access to private data. Despite the fact that such devices have uneven hardware capabilities and quantities of training data, and that network constraints and hardware failures can occur at any time, state of the art algorithms to conduct federated learning operate in synchronous rounds of communication. This work is a literature review of recent efforts to address this contradiction via novel protocols that relax the need to synchronize local training. Following a background study of the influences behind federated learning, six new research papers are surveyed to assess the progress of this new field towards developing robust and effective asynchronous federated learning algorithms.
			<br>
			<a href="spring20/Projects/Castello_Final_Presentation.pptx">Presentation</a> | 
			<a href="spring20/Projects/Castello_Final_Report.pdf">Report</a>

		</p>

		<h4> Learning a Fair Model under Privacy Constraints
		</h4>
		<p>
			<strong>Members</strong>: Cuong Tran <br>
			<strong>Summary</strong>:
			In this class project we study binary classification problems under three angles: accuracy, privacy and fairness. We explore how to maximize model’s accuracy given the constraints that the models’ prediction should not be discriminative towards certain protected groups. At the same time, we aim the model should be secured in term of protecting training data.
			<br>
			<a href="spring20/Projects/Cuong_slides.pdf">Presentation</a>
		</p>

		<h4> Improved StarGAN for Generating Adversarial Samples
		</h4>
		<p>
			<strong>Members</strong>: Jindi Wu, Mu Bai <br>
			<strong>Summary</strong>:
			We propose a solution of adversarial samples generation based on the StarGAN, which is an image-to-image translation  model  and  can  smoothly  generate  a  set  of new images from source images. The images generated by StarGAN could be adversarial samples for attacking deep neural network classifier. Our main aim is to attack the targeted model with specific output label and pass the corresponding smooth detection simultaneously.
			<br>
			<a href="spring20/Projects/jindi_mu_project.pptx">Presentation</a>
		</p>

		<h4> Constrained Deep Learning for Fast Approximation of Scheduling Problems
		</h4>
		<p>
			<strong>Members</strong>: James Kotary <br>
			<strong>Summary</strong>:
			Combinatorial  optimization  problems  in  planning  and scheduling are known to lack efficient solution methods,while demand for the frequent and accurate solution of such  problems  is  ever  increasing  across  several  engineering fields. Recent developments in machine learning promise new approaches for the fast approximation of constrained optimization problems at significantly reduced time-scales. This project will explore the application  of  constrained  deep  learning  to  build  a  system that predicts accurate solutions to challenging scheduling problems, with favorable properties not reflected in the original problem structures.
			<br>
			<a href="spring20/Projects/Kotary_Final_SPML.pdf">Presentation</a>
		</p>

		<h4> Privacy Preserving with Preference Elicitation Process
		</h4>
		<p>
			<strong>Members</strong>:  Pratik Ashok Paranjape and Raman Srivastava<br>
			<strong>Summary</strong>:
			Preference elicitation is the process of developing a decision support system to generate recommendations for a user. Privacy preserving refers to the system of publicly sharing information about a dataset by describing the  patterns  of  groups  within  the  dataset  without  dis-closing the information about individuals in the dataset. The idea of this project is to implement differential privacy in a recommendation system.
			<br>
			<a href="spring20/Projects/ProgressReport_05_07_2020.pdf">Presentation</a>
		</p>

<!--
		<h4>
			Creating a More Robust Payment Channel Network on Blockchain
		</h4>

 		<p>
 			<strong>Members</strong>: Ankit Khare <br>
			<strong>Summary</strong>:
			The growing adoption of crypto-currencies have brought about the advent of certain payment channel networks that are responsible for alleviating the problems of scaling that come hand in hand with the blockchain technology. Scaling of the blockchains are the biggest obstacles encountered  by  applications  which  makes  the  overall process slow and increases the cost of operation. Such networks are a viable solution with regards to increasing the speed of transactions made on the blockchain itself.  The  channels  within  the  networks  can  be  used to make unlimited bidirectional payments between two users. Such payment transfers are instantaneous and do not involve the blockchain itself. However, it is found that  such  networks  follow  the  laws  of  scale  free  and small world network architecture, and are hence prone to random and targeted attacks by adversaries aiming to disrupt the network. In this project we try to create a more robust network on the Ethereum blockchain payment channel network resistant to adversarial attacks by analysing budgets of the adversary and the payment network nodes.
			<br>
			<a href="spring20/Projects/Ankit_slides.key">Presentation</a>
		</p>
 -->

<!-- 		<h4> Adversarial Examples on Tabular Data
		</h4>
		<p>
			<strong>Members</strong>: Zuhal Altundal <br>
			<strong>Summary</strong>:
			The project develops adversarial examples for tabular data.
			<br>
			<a href="spring20/Projects/Zhual/smlprojreport.pdf">Presentation</a>
		</p>
 -->
		</div>

		<p></p>
		<strong>Links to Related Courses</strong> 
		<ul>
		  <li><a href="https://papernot.fr/teaching/f19-trustworthy-ml">Trustwothy Machine Learning</a></li>
		  <li><a href="https://secml.github.io">Security and Privacy of ML</a></li>
		</ul>
	</div>
	</div>

	<!-- Footer -->
	<div id="footer" class="shadow">
	  <p>&copy; 2018 - 2020 Ferdinando Fioretto | <a href="http://andreasviklund.com/templates/inland/">Template design</a> by <a href="http://andreasviklund.com/">andreasviklund.com</a><br /></p>
	  <p> Last Update: Jan. 2020
	</div>
	</div>

	<script type="text/javascript">
				$(window).load(function() {
					$('#slider').nivoSlider();
				});
	</script>

	<script>w3.includeHTML();</script>

	</body>
</html>


<!--
					P		R
 				1 2 3 4 5 6 7 8 9
Team Alderaan	x	2	x			[3]
Team Coruscant	x 2	x		x		[2]
Team Kamino		x	  3	  x		x	[4]
Team Mandalore	x	  x	3      	x	[5]
Team Naboo		4  	x 	  x	  		[1]
Team Onderon	  x	  	x 	  3		[8]
Team Tatooine	  x	  x	  2	  x		[6]
Team Yavin		  x	    x 	  x	2	[9]
 				1 2 3 4 5 6 7 8 9
 -->
