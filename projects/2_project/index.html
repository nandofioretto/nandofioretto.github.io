<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>SpecDiff-2 Scaling Diffusion Drafter Alignment | Ferdinando (Nando)  Fioretto</title>
    <meta name="author" content="Ferdinando (Nando)  Fioretto">
    <meta name="description" content="Faster LLM inference with discrete diffusion drafting and alignment.">
    <meta name="keywords" content="Ferdinando Fioretto, AI, ML, Optimization, differential privacy, fairness, trustworthy, university of virginia">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/projects/2_project/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ferdinando (Nando) </span>Fioretto</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/awards/">awards</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/group/">group</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/bio/">bio</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/assets/cv/cvFioretto.pdf">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">SpecDiff-2 Scaling Diffusion Drafter Alignment</h1>
            <p class="post-description">Faster LLM inference with discrete diffusion drafting and alignment.</p>
          </header>

          <article>
            <!-- ## SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding

**November 1, 2025**

**Jameson Sandler, Jacob K. Christopher, Thomas Hartvigsen, Nando Fioretto** -->

<h3 id="summary">Summary</h3>

<p>SpecDiff is a framework that accelerates Large Language Model inference by using non-autoregressive discrete diffusion models for draft generation, combined with novel train-time and test-time alignment strategies.</p>

<p>The method achieves an average 4.22x speed-up over vanilla autoregressive decoding and a 55% increase in tokens-per-second compared to prior speculative decoding baselines, all while maintaining the verifier model’s original accuracy.</p>

<h3 id="takeaways">Takeaways</h3>

<div class="takeaways-grid">
    <div class="takeaway-card">Discrete diffusion drafters enable genuine parallel multi-token generation.</div>
    <div class="takeaway-card">Alignment should optimize accepted streaks, not just token-level accuracy.</div>
    <div class="takeaway-card">Faster inference increases reasoning quality within fixed wall-time budgets.</div>
    <div class="takeaway-card">Position-wise marginals enable low-cost draft self-selection at test time.</div>
</div>

<h3 id="table-of-contents">Table of contents</h3>

<ul>
  <li><a href="#overview">Overview</a></li>
  <li><a href="#problem-statement-and-motivation">Problem statement and motivation</a></li>
  <li><a href="#methodology">Methodology</a></li>
  <li><a href="#key-results-and-performance">Key results and performance</a></li>
  <li><a href="#acceleration-compute-paradigm">Acceleration-compute paradigm</a></li>
  <li><a href="#ablation-studies-and-analysis">Ablation studies and analysis</a></li>
  <li><a href="#significance-and-impact">Significance and impact</a></li>
  <li><a href="#relevant-citations">Relevant citations</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>SpecDiff-2 addresses a critical challenge in large language model (LLM) inference: achieving
faster text generation without sacrificing accuracy. The framework combines discrete diffusion
models for parallel draft generation with specialized alignment techniques to significantly
accelerate LLM inference through speculative decoding.</p>

<div class="project-figure project-figure--float-left">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/specdiff2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/specdiff2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/specdiff2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/projects/specdiff2.jpeg" class="img-fluid rounded z-depth-1 project-figure__img" width="auto" height="auto" title="SpecDiff-2 speed-ups across verifier models" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>


<div class="caption">
    <strong>Figure 1</strong>: SpecDiff-2 achieves superior speed-ups compared to existing speculative decoding
    methods across different verifier models, with improvements of over 30% compared to EAGLE-2.
</div>
</div>

<h2 id="problem-statement-and-motivation">Problem Statement and Motivation</h2>

<p>Current LLM inference is inherently slow due to the autoregressive nature of text generation, where tokens are produced sequentially one at a time. Speculative decoding has emerged as a promising solution, using a smaller “drafter” model to propose multiple tokens in parallel, which are then verified by a larger “verifier” model. However, existing approaches face two critical bottlenecks:</p>

<ul>
  <li>
<strong>Autoregressive Dependency in Drafting</strong>: Most drafter models are themselves autoregressive, generating tokens sequentially and limiting the parallelism benefits. This sequential dependency adds latency to the drafting process, especially for longer draft sequences.</li>
  <li>
<strong>Drafter-Verifier Misalignment</strong>: When the drafter’s proposals don’t align well with the verifier’s preferences, many tokens get rejected, requiring regeneration and negating potential speed gains. This misalignment is particularly problematic when using diffusion-based drafters, as they learn joint distributions that can be miscalibrated with autoregressive verifiers.</li>
</ul>

<p>SpecDiff-2 tackles both bottlenecks simultaneously through a unified framework that leverages discrete diffusion models for parallel drafting and introduces novel alignment mechanisms to improve acceptance rates.</p>

<h2 id="methodology">Methodology</h2>

<h3 id="discrete-diffusion-for-parallel-drafting">Discrete diffusion for parallel drafting</h3>

<p>SpecDiff-2 employs Masked-Discrete Diffusion Models (MDMs) as drafters to eliminate autoregressive dependencies. Unlike traditional autoregressive models, diffusion models generate text by iteratively refining corrupted sequences over a fixed number of denoising steps. Critically, each denoising step updates all token positions in parallel.</p>

<p>The process works by extending a prefix \(s\) with \(\gamma\) masked tokens [MASK], then using a single denoising step to generate a joint proposal:</p>

\[x_{1:\gamma} \sim Q_{diff}(\cdot \mid s \circ [MASK]_{1:\gamma})\]

<p>This parallel generation means drafting cost depends primarily on the fixed number of denoising steps rather than the draft length, providing substantial efficiency gains for longer sequences.</p>

<h3 id="dual-alignment-strategy">Dual alignment strategy</h3>

<div class="project-figure">
    <div class="row">
        <div class="col-md-6 mt-3 mt-md-0">
            <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/specdiff2-fig2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/specdiff2-fig2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/specdiff2-fig2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/projects/specdiff2-fig2.jpeg" class="img-fluid rounded z-depth-1 project-figure__img" width="auto" height="auto" title="Train-time streak-distillation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

        </div>
        <div class="col-md-6 mt-3 mt-md-0">
            <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/specdiff2-fig3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/specdiff2-fig3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/specdiff2-fig3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/projects/specdiff2-fig3.jpeg" class="img-fluid rounded z-depth-1 project-figure__img" width="auto" height="auto" title="Test-time self-selection" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

        </div>
    </div>
</div>
<div class="caption">
    <strong>Figure 2 (left)</strong>: Train-time streak-distillation aligns the diffusion drafter with the verifier by
    optimizing for long accepted token sequences across the entire draft window.
    <strong>(Right)</strong>:
    Test-time self-selection leverages diffusion parallelism to generate multiple draft candidates
    and select the most promising one using verifier-based scoring.
</div>

<p><strong>Train-time streak-distillation.</strong> 
The drafter is optimized for long accepted streaks, not just individual token likelihoods, using a greedy acceptance proxy
\(\Pr(accept\ x_i \mid s) = P(x_i \mid s)\). In practice, the objective treats the verifier as a
teacher and encourages the diffusion drafter to generate sequences that maximize expected
committed tokens across the full draft window. This shifts training away from myopic
token-level matches and toward sequences that the verifier is likely to accept end-to-end,
which directly improves throughput.</p>

\[ext{Tokens}_{\text{Draft}}(\gamma, s) = \mathbb{E}_{x \sim Q_{diff}(\cdot \mid s)}\left[\sum_{i=1}^{\gamma} \prod_{j=1}^{i} P(x_j \mid s_{&lt;j})\right]\]

<p>The training uses verifier-guided trajectories to estimate gradients, reinforcing long
accepted streaks instead of only the first few tokens. The result is a drafter that is
better calibrated to the verifier across the entire draft horizon.</p>

<p><strong>Test-time self-selection acceptance.</strong> Position-wise marginals allow sampling \(K\) joint drafts from a single denoising pass. Candidates are ranked using a streak-oriented verifier score. Because diffusion updates all positions in parallel, multiple candidate drafts can be generated with minimal overhead, and the highest-scoring draft is selected for verification to maximize expected committed tokens.</p>

\[ext{Score}(x^{(k)}) = \sum_{i=1}^{\gamma} \prod_{j=1}^{i} P(x^{(k)}_j \mid s_{&lt;j})\]

<p>This test-time selection complements training alignment: higher draft diversity at larger \(K\) improves acceptance rates without changing the verifier or sacrificing output quality.</p>

<h2 id="key-results-and-performance">Key Results and Performance</h2>

<p>SpecDiff-2 demonstrates significant improvements across multiple benchmarks and metrics:</p>

<ul>
  <li>
    <p><strong>Speed-up Performance</strong>: The method achieves an average 4.22x speed-up across all tested settings, representing over 30% improvement compared to EAGLE-2. On specialized tasks like code generation, speed-ups reach 5.51x compared to vanilla autoregressive generation.</p>
  </li>
  <li>
    <p><strong>Accepted streak length</strong>: SpecDiff-2 produces substantially longer accepted sequences. For example, with Qwen2.5-72B at temperature 0, it achieves 5.98 tokens per draft on average compared to EAGLE-2’s 4.41 tokens.</p>
  </li>
  <li>
    <p><strong>Scaling Properties</strong>: The self-selection mechanism shows smooth test-time scaling, with increasing the number of parallel drafts from 1 to 8 yielding up to 20% additional throughput, particularly effective at higher drafter temperatures where draft diversity is greater.</p>
  </li>
</ul>

<div class="project-figure">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/specdiff2-fig4-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/specdiff2-fig4-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/specdiff2-fig4-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/projects/specdiff2-fig4.jpeg" class="img-fluid rounded z-depth-1 project-figure__img" width="auto" height="auto" title="Self-selection scaling with K drafts" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>


<div class="caption">
    <strong>Figure 4</strong>: Test-time self-selection scales smoothly with the number of parallel drafts,
    yielding up to 20% additional throughput at higher drafter temperatures.
</div>
</div>

<h2 id="acceleration-compute-paradigm">Acceleration-Compute Paradigm</h2>

<p>A significant contribution is introducing “acceleration-compute” as a new scaling dimension for LLMs. The research demonstrates that faster inference directly translates to higher accuracy on complex reasoning tasks within fixed wall-time budgets. On Math500 with Chain-of-Thought prompting, SpecDiff-2 achieves a +63% boost in accuracy over vanilla models and +11% increase over unaligned SpecDiff within a 15-second reasoning budget.
This paradigm shift suggests that optimizing for throughput isn’t just about speed—it’s about improving the effective intelligence of LLMs within practical time constraints by enabling more reasoning steps.</p>

<div class="project-figure">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/specdiff2-fig5-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/specdiff2-fig5-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/specdiff2-fig5-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/projects/specdiff2-fig5.jpeg" class="img-fluid rounded z-depth-1 project-figure__img" width="auto" height="auto" title="Acceleration-compute improves reasoning" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>


<div class="caption">
    <strong>Figure 5</strong>: Faster inference enables better reasoning within fixed time budgets. SpecDiff-2
    provides the highest accuracy gains among acceleration methods.
</div>
</div>

<h2 id="ablation-studies-and-analysis">Ablation Studies and Analysis</h2>

<p><strong>Individual Component Analysis</strong>: Ablations reveal that streak-distillation alone provides substantial gains, with approximately +30% speed-up improvement over base diffusion drafters. The combination of both alignment mechanisms results in 40-50% performance improvement over unaligned SpecDiff.</p>

<p><strong>Position-wise Acceptance Analysis</strong>: The research includes detailed analysis showing how acceptance rates degrade with distance from the prefix. Streak-distillation significantly improves this degradation pattern compared to standard autoregressive distillation, maintaining higher acceptance rates throughout the draft window.</p>

<div class="project-figure">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/specdiff2-fig6-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/specdiff2-fig6-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/specdiff2-fig6-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/projects/specdiff2-fig6.jpeg" class="img-fluid rounded z-depth-1 project-figure__img" width="auto" height="auto" title="Streak-distillation training progression" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>


<div class="caption">
    <strong>Figure 6</strong>: Streak-distillation training yields monotonic speed-up improvements with
    increasing training steps.
</div>
</div>

<h2 id="significance-and-impact">Significance and Impact</h2>

<p>SpecDiff-2 establishes a new state-of-the-art for lossless LLM inference acceleration, with several important implications:</p>

<ul>
  <li>
<strong>Architectural Innovation</strong>: The work expands the design space for speculative decoding by demonstrating the superiority of discrete diffusion models over traditional autoregressive drafters, opening new research directions for non-autoregressive generation methods.</li>
  <li>
<strong>Practical Deployment Benefits</strong>: The significant speed improvements without accuracy loss enable more responsive user applications, reduced computational costs, and feasibility of real-time LLM systems. The ability to generate more tokens within fixed time budgets allows for deeper reasoning capabilities.</li>
  <li>
<strong>Research Paradigm</strong>: The introduction of “acceleration-compute” as a scaling factor provides a new lens for understanding LLM performance optimization, suggesting that inference efficiency improvements can directly enhance problem-solving capabilities rather than just reducing latency.</li>
</ul>

<p>The comprehensive approach of jointly addressing both drafting latency and alignment challenges through specialized techniques tailored for diffusion models represents a significant advancement in making powerful LLMs more practical and accessible for real-world applications.</p>

<h2 id="relevant-citations">Relevant citations</h2>

<ul>
  <li>Leviathan, Y., Kalman, M., and Matias, Y. <strong>Fast inference from transformers via speculative decoding.</strong> ICML 2023.</li>
</ul>

<p>This paper is foundational as it introduces the speculative decoding paradigm, the core “draft-then-verify” framework that SpecDiff-2 is built upon. It defines the lossless acceptance rule that is central to the paper’s method and the entire field of speculative decoding acceleration.</p>

<ul>
  <li>Christopher, J. K., Bartoldson, B. R., Ben-Nun, T., Cardei, M., Kailkhura, B., and Fioretto, F. 
<strong>Speculative diffusion decoding: Accelerating language generation through diffusion.</strong> NAACL 2025</li>
</ul>

<p>This work, referred to as SpecDiff, is the direct predecessor to the presented paper, SpecDiff-2. It introduced the novel concept of using a non-autoregressive diffusion language model as a drafter, which is the central mechanism that SpecDiff-2 enhances with its new alignment techniques.</p>

<ul>
  <li>
    <p>Li, Y., Wei, F., Zhang, C., and Zhang, H. <strong>Eagle-2: Faster inference of language models with dynamic draft trees</strong>. arXiv preprint arXiv:2406.16858, 2024.</p>
  </li>
  <li>
    <p>Zhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Ros-tamizadeh, A., Kumar, S., Kagy, J.-F., and Agarwal, R. <strong>Distillspec: Improving speculative decoding via knowledge distillation</strong>. arXiv preprint arXiv:2310.08461, 2023.</p>
  </li>
</ul>

          </article><div id="giscus_thread" style="max-width: 1060px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "alshedivat/al-folio",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2026 Ferdinando (Nando)  Fioretto. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: February 10, 2026.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
